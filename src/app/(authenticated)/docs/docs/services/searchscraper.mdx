---
title: 'SearchScraper'
description: 'Search and extract information from multiple web sources using AI'
icon: 'magnifying-glass'
---

<Frame>
  <img src="/services/images/searchscraper-banner.png" alt="SearchScraper Service" />
</Frame>

## Overview

SearchScraper is our advanced LLM-powered search service that intelligently searches and aggregates information from multiple web sources. Using state-of-the-art language models, it understands your queries and extracts relevant information across the web, providing comprehensive answers with full source attribution.

<Note>
Try SearchScraper instantly in our [interactive playground](https://dashboard.scrapegraphai.com/) - no coding required!
</Note>

## Getting Started

### Quick Start

<CodeGroup>

```python Python
from scrapegraph_py import Client

client = Client(api_key="your-api-key")

response = client.searchscraper(
    user_prompt="What are the key features and pricing of ChatGPT Plus?"
)
```

```javascript JavaScript
import { searchScraper } from 'scrapegraph-js';

const apiKey = 'your-api-key';
const prompt = 'What is the latest version of Python and what are its main features?';

try {
  const response = await searchScraper(apiKey, prompt);
  console.log(response);
} catch (error) {
  console.error(error);
}
```

```bash cURL
curl -X 'POST' \
  'https://api.scrapegraphai.com/v1/searchscraper' \
  -H 'accept: application/json' \
  -H 'SGAI-APIKEY: your-api-key' \
  -H 'Content-Type: application/json' \
  -d '{
  "user_prompt": "Search for information"
}'
```

</CodeGroup>

#### Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| apiKey | string | Yes | The ScrapeGraph API Key. |
| prompt | string | Yes | A textual description of what you want to achieve. |
| schema | object | No | The Pydantic or Zod object that describes the structure and format of the response |

<Note>
Get your API key from the [dashboard](https://dashboard.scrapegraphai.com)
</Note>

<Accordion title="Example Response" icon="terminal">
```json
{
  "request_id": "sg-req-abc123",
  "status": "completed",
  "user_prompt": "What are the key features and pricing of ChatGPT Plus?",
  "result": {
    "product": {
      "name": "ChatGPT Plus",
      "description": "Premium version of ChatGPT with advanced features and capabilities",
      "target_audience": "Power users and professionals requiring advanced AI capabilities"
    },
    "features": [
      {
        "name": "GPT-4 Access",
        "description": "Access to the latest GPT-4 language model"
      },
      {
        "name": "Response Speed",
        "description": "Faster response times compared to free tier"
      },
      {
        "name": "Priority Access",
        "description": "Guaranteed access during peak usage times"
      },
      {
        "name": "New Features",
        "description": "Early access to new features and improvements"
      },
      {
        "name": "Plugin Support",
        "description": "Access to third-party plugins and integrations"
      }
    ],
    "pricing": {
      "plans": [
        {
          "name": "Plus Subscription",
          "price": {
            "amount": 20,
            "currency": "USD",
            "period": "monthly"
          },
          "features": [
            "GPT-4 access",
            "Faster response speed",
            "Priority access during peak times",
            "Early feature access",
            "Plugin support"
          ]
        }
      ]
    },
    "availability": {
      "regions": [
        "United States",
        "European Union",
        "United Kingdom",
        "Most other countries"
      ],
      "restrictions": [
        "Not available in sanctioned countries",
        "Requires credit card for subscription"
      ]
    }
  },
  "reference_urls": [
    "https://openai.com/chatgpt",
    "https://openai.com/blog/chatgpt-plus",
    "https://help.openai.com/en/articles/6825453-chatgpt-plus-plan"
  ],
  "error": ""
}
```

The response includes:
- `request_id`: Unique identifier for tracking your request
- `status`: Current status of the search ("completed", "running", "failed")
- `result`: The extracted data in structured JSON format
- `reference_urls`: Source URLs for verification
- `error`: Error message (if any occurred during search)
</Accordion>

## Key Features

<CardGroup cols={2}>
  <Card title="Multi-Source Search" icon="globe">
    Intelligent search across multiple reliable web sources
  </Card>
  <Card title="AI Understanding" icon="brain">
    Advanced LLM models for accurate information extraction
  </Card>
  <Card title="Structured Output" icon="table">
    Clean, structured data in your preferred format
  </Card>
  <Card title="Source Attribution" icon="link">
    Full transparency with reference URLs
  </Card>
</CardGroup>

## Use Cases

### Research & Analysis
- Academic research and fact-finding
- Market research and competitive analysis
- Technology trend analysis
- Industry insights gathering

### Data Aggregation
- Product research and comparison
- Company information compilation
- Price monitoring across sources
- Technology stack analysis

### Content Creation
- Fact verification and citation
- Content research and inspiration
- Data-driven article writing
- Knowledge base building

<Note>
Want to learn more about our AI-powered search technology? Visit our [main website](https://scrapegraphai.com) to discover how we're revolutionizing web research.
</Note>

## Other Functionality

### Retrieve a previous request

If you know the response id of a previous request you made, you can retrieve all the information.

<CodeGroup>

```javascript JavaScript
import { getSearchScraperRequest } from 'scrapegraph-js';

const apiKey = 'your_api_key';
const requestId = 'ID_of_previous_request';

try {
  const requestInfo = await getSearchScraperRequest(apiKey, requestId);
  console.log(requestInfo);
} catch (error) {
  console.error(error);
}
```

```bash cURL
curl -X 'POST' \
  'https://api.scrapegraphai.com/v1/searchscraper' \
  -H 'accept: application/json' \
  -H 'SGAI-APIKEY: sgai-********************' \
  -H 'Content-Type: application/json' \
  -d '{
  "user_prompt": "Search for information"
}'
```

</CodeGroup>

#### Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| apiKey | string | Yes | The ScrapeGraph API Key. |
| requestId | string | Yes | The request ID associated with the output of a previous searchScraper request. |


### Custom Schema Example

Define exactly what data you want to extract using Pydantic or Zod:

<CodeGroup>

```python Python
from pydantic import BaseModel, Field
from typing import List

class CompanyProfile(BaseModel):
    name: str = Field(description="Company name")
    description: str = Field(description="Brief company description")
    founded_year: str = Field(description="Year the company was founded")
    headquarters: str = Field(description="Company headquarters location")
    employees: str = Field(description="Number of employees")
    industry: str = Field(description="Primary industry")
    products: List[str] = Field(description="Main products or services")
    competitors: List[str] = Field(description="Major competitors")
    market_share: str = Field(description="Company's market share")
    revenue: str = Field(description="Annual revenue")
    tech_stack: List[str] = Field(description="Technologies used by the company")

response = client.searchscraper(
    user_prompt="Find comprehensive information about OpenAI",
    output_schema=CompanyProfile
)
```

```typescript JavaScript
import { searchScraper } from 'scrapegraph-js';
import { z } from 'zod';

const apiKey = "your_api_key";
const prompt = 'What is the latest version of Python and what are its main features?';

const CompanyProfile = z.object({
  name: z.string().describe('Company name'),
  description: z.string().describe('Brief company description'),
  foundedYear: z.string().describe('Year the company was founded'),
  headquarters: z.string().describe('Company headquarters location'),
  employees: z.string().describe('Number of employees'),
  industry: z.string().describe('Primary industry'),
  products: z.array(z.string()).describe('Main products or services'),
  competitors: z.array(z.string()).describe('Major competitors'),
  marketShare: z.string().describe('Company\'s market share'),
  revenue: z.string().describe('Annual revenue'),
  techStack: z.array(z.string()).describe('Technologies used by the company')
});

try {
  const response = await searchScraper(apiKey, prompt, schema);
  console.log(response.result);
} catch (error) {
  console.error(error);
}
```

</CodeGroup>

### Advanced Schema Usage

The schema system in SearchScraper is a powerful way to ensure you get exactly the data structure you need. Here are some advanced techniques for using schemas effectively:

#### Nested Schemas

You can create complex nested structures to capture hierarchical data:

<CodeGroup>

```python Python
from pydantic import BaseModel, Field
from typing import List, Optional

class Author(BaseModel):
    name: str = Field(description="Author's full name")
    bio: Optional[str] = Field(description="Author's biography")
    expertise: List[str] = Field(description="Areas of expertise")

class Article(BaseModel):
    title: str = Field(description="Article title")
    content: str = Field(description="Main article content")
    author: Author = Field(description="Article author information")
    publication_date: str = Field(description="Date of publication")
    tags: List[str] = Field(description="Article tags or categories")

response = client.searchscraper(
    user_prompt="Find the latest AI research articles",
    output_schema=Article
)
```

```typescript JavaScript
import { z } from 'zod';

const Author = z.object({
  name: z.string().describe("Author's full name"),
  bio: z.string().optional().describe("Author's biography"),
  expertise: z.array(z.string()).describe("Areas of expertise")
});

const Article = z.object({
  title: z.string().describe("Article title"),
  content: z.string().describe("Main article content"),
  author: Author.describe("Article author information"),
  publicationDate: z.string().describe("Date of publication"),
  tags: z.array(z.string()).describe("Article tags or categories")
});

const response = await searchScraper(apiKey, prompt, Article);
```

</CodeGroup>

#### Schema Validation Rules

Enhance data quality by adding validation rules to your schema:

<CodeGroup>

```python Python
from pydantic import BaseModel, Field, validator
from typing import List
from datetime import datetime

class ProductInfo(BaseModel):
    name: str = Field(description="Product name")
    price: float = Field(description="Product price", gt=0)
    currency: str = Field(description="Currency code", max_length=3)
    release_date: str = Field(description="Product release date")
    
    @validator('currency')
    def validate_currency(cls, v):
        if len(v) != 3 or not v.isupper():
            raise ValueError('Currency must be a 3-letter uppercase code')
        return v
        
    @validator('release_date')
    def validate_date(cls, v):
        try:
            datetime.strptime(v, '%Y-%m-%d')
            return v
        except ValueError:
            raise ValueError('Date must be in YYYY-MM-DD format')
```

```typescript JavaScript
import { z } from 'zod';

const ProductInfo = z.object({
  name: z.string().min(1).describe("Product name"),
  price: z.number().positive().describe("Product price"),
  currency: z.string().length(3).toUpperCase()
    .describe("Currency code"),
  releaseDate: z.string().regex(/^\d{4}-\d{2}-\d{2}$/)
    .describe("Product release date")
});
```

</CodeGroup>

### Quality Improvement Tips

To get the highest quality results from SearchScraper, follow these best practices:

#### 1. Detailed Field Descriptions

Always provide clear, detailed descriptions for each field in your schema:

```python
class CompanyInfo(BaseModel):
    revenue: str = Field(
        description="Annual revenue in USD, including the year of reporting"
        # Good: "Annual revenue in USD, including the year of reporting"
        # Bad: "Revenue"
    )
    market_position: str = Field(
        description="Company's market position including market share percentage and rank among competitors"
        # Good: "Company's market position including market share percentage and rank among competitors"
        # Bad: "Position"
    )
```

#### 2. Structured Prompts

Combine schemas with well-structured prompts for better results:

```python
response = client.searchscraper(
    user_prompt="""
    Find information about Tesla's electric vehicles with specific focus on:
    - Latest Model 3 and Model Y specifications
    - Current pricing structure
    - Available customization options
    - Delivery timeframes
    Please include only verified information from official sources.
    """,
    output_schema=TeslaVehicleInfo
)
```

#### 3. Data Validation

Implement comprehensive validation to ensure data quality:

```python
from pydantic import BaseModel, Field, validator
from typing import List, Optional
from datetime import datetime

class MarketData(BaseModel):
    timestamp: str = Field(description="Data timestamp in ISO format")
    value: float = Field(description="Market value")
    confidence_score: float = Field(description="Confidence score between 0 and 1")
    
    @validator('timestamp')
    def validate_timestamp(cls, v):
        try:
            datetime.fromisoformat(v)
            return v
        except ValueError:
            raise ValueError('Invalid ISO timestamp format')
    
    @validator('confidence_score')
    def validate_confidence(cls, v):
        if not 0 <= v <= 1:
            raise ValueError('Confidence score must be between 0 and 1')
        return v
```

#### 4. Error Handling

Implement robust error handling for schema validation:

```python
try:
    response = client.searchscraper(
        user_prompt="Find market data for NASDAQ:AAPL",
        output_schema=MarketData
    )
    validated_data = MarketData(**response.result)
except ValidationError as e:
    print(f"Data validation failed: {e.json()}")
    # Implement fallback logic or error reporting
except Exception as e:
    print(f"An error occurred: {str(e)}")
```

### Async Support

Example of using the async searchscraper functionality to search for information concurrently:

```python
import asyncio
from scrapegraph_py import AsyncClient
from scrapegraph_py.logger import sgai_logger

sgai_logger.set_logging(level="INFO")

async def main():
    # Initialize async client
    sgai_client = AsyncClient(api_key="your-api-key-here")

    # List of search queries
    queries = [
        "What is the latest version of Python and what are its main features?",
        "What are the key differences between Python 2 and Python 3?",
        "What is Python's GIL and how does it work?",
    ]

    # Create tasks for concurrent execution
    tasks = [sgai_client.searchscraper(user_prompt=query) for query in queries]

    # Execute requests concurrently
    responses = await asyncio.gather(*tasks, return_exceptions=True)

    # Process results
    for i, response in enumerate(responses):
        if isinstance(response, Exception):
            print(f"\nError for query {i+1}: {response}")
        else:
            print(f"\nSearch {i+1}:")
            print(f"Query: {queries[i]}")
            print(f"Result: {response['result']}")
            print("Reference URLs:")
            for url in response["reference_urls"]:
                print(f"- {url}")

    await sgai_client.close()

if __name__ == "__main__":
    asyncio.run(main())
```

## Integration Options

### Official SDKs
- [Python SDK](/sdks/python) - Perfect for data science and backend applications
- [JavaScript SDK](/sdks/javascript) - Ideal for web applications and Node.js

### AI Framework Integrations
- [LangChain Integration](/integrations/langchain) - Use SearchScraper in your LLM workflows
- [LlamaIndex Integration](/integrations/llamaindex) - Build powerful search and QA systems
- [CrewAI Integration](/integrations/crewai) - Create AI agents with search capabilities

## Best Practices

### Query Optimization
1. Be specific in your prompts
2. Use descriptive queries
3. Include relevant context
4. Specify time-sensitive requirements

### Schema Design
- Start with essential fields
- Use appropriate data types
- Add field descriptions
- Make optional fields nullable
- Group related information

### Rate Limiting
- Implement reasonable delays between requests
- Use async clients for better performance
- Monitor your [API usage](/dashboard/overview)

## Example Projects

Check out our [cookbook](/cookbook/introduction) for real-world examples:
- [Company Research](/cookbook/examples/company-info)
- [Market Analysis](/cookbook/examples/research-agent)
- [Technology Trends](/cookbook/examples/github-trending)
- [News Aggregation](/cookbook/examples/wired)

## API Reference

For detailed API documentation, see:
- [Start Search](/api-reference/endpoint/searchscraper/start)
- [Get Search Status](/api-reference/endpoint/searchscraper/get-status)

## Support & Resources

<CardGroup cols={2}>
  <Card title="Documentation" icon="book" href="/introduction">
    Comprehensive guides and tutorials
  </Card>
  <Card title="API Reference" icon="code" href="/api-reference/introduction">
    Detailed API documentation
  </Card>
  <Card title="Community" icon="discord" href="https://discord.gg/uJN7TYcpNa">
    Join our Discord community
  </Card>
  <Card title="GitHub" icon="github" href="https://github.com/ScrapeGraphAI">
    Check out our open-source projects
  </Card>
</CardGroup>

<Card title="Ready to Start?" icon="rocket" href="https://dashboard.scrapegraphai.com">
  Sign up now and get your API key to begin searching and extracting data with SearchScraper!
</Card> 